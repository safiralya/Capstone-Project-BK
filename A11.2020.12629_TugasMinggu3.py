# -*- coding: utf-8 -*-
"""Heart Disease with Hungarian Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ri1vpz5c_Q14X3tH1zrnXfs87yTeo8D2

#1) Pengumpulan Data

Dataset yang dikumpulkan bersumber dari UC Irvine Machine Learning Repository, yaitu repositori online yang terkenal untuk kumpulan data yang digunakan oleh komunitas pembelajaran mesin dan data science. Dataset yang digunakan adalah dataset yang membahas mengenai Heart Disease yaitu database dari *Hungary* dengan nama file "hungarian.data" dengan link sebagai berikut: https://archive.ics.uci.edu/dataset/45/heart+disease.

Dataset ini mencakup data pasien yang dikumpulkan dari *Hungary* yang
berisi variabel atau informasi klinis dan fisik pasien yang berkaitan dengan penyakit jantung seperti umur, jenis kelamin, tekanan darah, kadar kolesterol, hasil elektrokardiogram, dan lainnya.Diharapkan dengan digunakannya dataset ini dapat digunakan dan berkontribusi untuk upaya memahami dan  memprediksi penyakit jantung, dari yang sehat sampai pada tingkatan penyakit jantungnya.

#2) Menelaah Data

Pada tahap menelaah data dilakukan pemeriksaan awal untuk mamahami struktur dan isi dalam data.

Import library yang digunakan untuk menelaah proyek ini yaitu:
1. **pandas** yang digunakan untuk analisis data
2. **re** atau **RegEx** digunakan untuk pencarian, manipulasi, dan validasi string
3. **numpy** digunakan untuk berbagai fungsi matematika tingkat tinggu untuk operasi pada array
4. **itertools** digunakan untuk bekerja dengan iterasi data
"""

import pandas as pd
import re
import numpy as np
import itertools

"""## Load Data"""

# cara 1 -> upload dataset pada google colab, namun kelemahannya ketika runtime terputus perlu dilakukan upload ulang dataset
# dir = 'hungarian.data'

# cara 2 -> upload dataset pada gdrive
# dir adalah variabel yang digunakan untuk menyimpan data dengan path dari google colab

dir = '/content/drive/MyDrive/KULIAH/Semester 7/BIMBINGAN KARIER (4701)/Sertifikasi/Database/hungarian.data'

# mengizinkan Gcollab mengakses gdrive

from google.colab import drive
drive.mount('/content/drive')

"""Fungsi *ls* digunakan untuk menampilkan isi folder, tujuannya untuk memastikan bahwa file data yang diperlukan untuk analisis sudah tersedia di lokasi yang benar"""

!ls '/content/drive/MyDrive/KULIAH/Semester 7/BIMBINGAN KARIER (4701)/Sertifikasi/Database'

# iterasi untuk membaca dataset per-10 baris, karena dilihat berdasarkan pola iterasi yang sama setiap 10 baris

with open(dir, encoding='Latin1') as file:
  lines = [line.strip() for line in file]

lines [0:10]

# iterasi sesuai jumlah kolom dan baris, di mana kolom pada dataset berjumlah 76

data = itertools.takewhile(
    lambda x: len(x) == 76,
    (' '.join(lines[i:(i + 10)]).split() for i in range(0, len(lines), 10))
)

df = pd.DataFrame.from_records(data)

df.head()

# mengetahui ringkasan informasi dalam dataframe

df.info()

"""Dataset yang digunakan dalam proyek ini memiliki karakteristik khusus di mana nilai null direpresentasikan dengan angka -9.0. Oleh karena itu, diperlukan penghapusan fitur-fitur tertentu pada data dengan tipe data object atau string."""

# menghapus kolom yang berisikan ID pasien, karena merupakan data kategorikal

df = df.iloc[:,:-1]
df = df.drop(df.columns[0], axis=1)

# mengubah tipe data file dataset menjadi tipe data float sesuai dengan nilai null yang disimbolkan dengan angka -9.0

df = df.astype(float)

df.info()

"""#3) Validasi Data

Pada tahap validasi data dilakukan pemeriksaan lebih lanjut dengan pengecekan dan penanganan nilai yang hilang.
"""

# mengubah nilai -9.0 menjadi nilai null value

df.replace(-9.0, np.nan, inplace=True)

# menghitung jumlah null

df.isnull().sum()

df.head()

df.info()

"""#4) Menentukan Object Data

Memilih 14 fitur yang akan digunakan sesuai dengan deksripsi dataset
"""

df_selected = df.iloc[:, [1, 2, 7, 8, 10, 14, 17, 30, 36, 38, 39, 42, 49, 56]]
df_selected.head()

df_selected.tail()

df_selected.info()

# mengganti nama kolom sesuai dengan 14 nama kolom yang ada pada deskripsi dataset

column_mapping = {
    2: 'age',
    3: 'sex',
    8: 'cp',
    9: 'trestbps',
    11: 'chol',
    15: 'fbs',
    18: 'restecg',
    31: 'thalach',
    37: 'exang',
    39: 'oldpeak',
    40: 'slope',
    43:'ca',
    50: 'thal',
    57: 'target'
}

df_selected.rename(columns=column_mapping, inplace=True)

df_selected.info()

# menghitung jumlah fitur pada dataset

df_selected.value_counts()

"""#5) Membersihkan Data

Tahapan membersihkan data bertujuan untuk memperbaiki atau menghilangkan data yang tidak akurat, tidak lengkap, atau tidak relevan dalam dataset. Beberapa proses yang dilakukan yaitu seperti identifikasi dan penanganan nilai yang hilang serta menghapus data yang duplikat dan mencari korelasi antar data atau fitur.

## Null Values
"""

# menghitung jumlah null values

df_selected.isnull().sum()

"""Terlihat pada jumlah null values pada tiap fitur bahwa terdapat beberapa fitur yang jumlah null values-nya hampir 90% dari datanya, sehingga perlu dilakukan penghapusan fitur."""

# menghapus fitur dengan jumlah null values hampir 90%

columns_to_drop = ['ca', 'slope','thal']
df_selected = df_selected.drop(columns_to_drop, axis=1)

df_selected.isnull().sum()

df_selected['chol']

"""Kemudian, terlihat juga masih ada beberapa fitur yang memiliki null values. Maka dari itu, perlu dilakukan pengisian null values dengan nilai mean dari setiap fitur. Penggunaan nilai mean sebagai pengisian null values dilakukan untuk membantu menjaga distribusi keseluruhan data sehingga tidak mempengaruhi analisis dan model pembelajaran yang akan digunakan."""

# menghapus seluruh null values pada fitur

meanTBPS = df_selected['trestbps'].dropna()
meanChol = df_selected['chol'].dropna()
meanfbs = df_selected['fbs'].dropna()
meanRestCG = df_selected['restecg'].dropna()
meanthalach = df_selected['thalach'].dropna()
meanexang = df_selected['exang'].dropna()

# mengubah tipe data pada variabel mean... menjadi float

meanTBPS = meanTBPS.astype(float)
meanChol = meanChol.astype(float)
meanfbs = meanfbs.astype(float)
meanthalach = meanthalach.astype(float)
meanexang = meanexang.astype(float)
meanRestCG = meanRestCG.astype(float)

# menghitung rata-rata dari fitur dan kemudian membulatkan hasil perhitungan

meanTBPS = round(meanTBPS.mean())
meanChol = round(meanChol.mean())
meanfbs = round(meanfbs.mean())
meanthalach = round(meanthalach.mean())
meanexang = round(meanexang.mean())
meanRestCG = round(meanRestCG.mean())

# mengisi null values menjadi nilai mean yang telah ditentukan

fill_values = {'trestbps': meanTBPS, 'chol': meanChol, 'fbs': meanfbs,
              'thalach':meanthalach,'exang':meanexang,'restecg':meanRestCG}
df_clean = df_selected.fillna(value=fill_values)

df_clean.info()

df_clean.isnull().sum()

"""Akhirnya, seluruh null values tidak ada pada dataset dan dapat dilakukan langkah selanjutnya.

## Duplicate Data
"""

# pengecekan data duplikat

duplicate_rows = df_clean.duplicated()
df_clean[duplicate_rows]

# cetak semua data duplikat

print('All Duplicate Rows:')
df_clean[df_clean.duplicated(keep=False)]

# menghapus data duplikat

df_clean = df_clean.drop_duplicates()
print('All Duplicate Rows:')
df_clean[df_clean.duplicated(keep=False)]

df_clean.head()

"""## Data Correlation

Import library yang digunakan dalam menampilan korelasi data, yaitu:
1. **seaborn** yang digunakan untuk menggambar grafik statistik yang menarik dan informatif
2. **matplotlib** yang digunakan untuk membuat berbagai jenis grafik
"""

import seaborn as sns
import matplotlib.pyplot as plt

cor_mat = df_clean.corr()
fig,ax = plt.subplots(figsize=(15,10))
sns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=".3f")

"""Dapat dilihat bahwa diagonal utama pada heatmap, dari kiri atas ke kanan bawah selalu memiliki nilai korelasi 1, karena ini menunjukkan korelasi setiap fitur dengan dirinya sendiri. Selanjutnya, fitur cp, thalach, dan oldpeak memiliki korelasi positif yang cukup kuat dengan fitur target, yang menunjukkan bahwa 3 fitur tersebut 'mungkin' berhubungan dengan fitur target. Selain itu fitur exang memiliki korelasi negatif dengan fitur target, yang juga menunjukkan 'kemungkinan' adanya hubungan dengan kondisi target.

#6) Konstruksi Data

Tahapan selanjutnya yaitu Konstruksi Data dengan melakukan antara lain memisahkan kolom fitur dan kolom target, menyeimbangkan data target, normalisasi data, dan yang terakhir membagi data menjadi data train dan data test.

## Memisahkan Data Target
"""

# melakukan penghapusan kolom target untuk variabel x dan mengambil kolom target saja untuk variabel y

x = df_clean.drop('target', axis=1).values
y = df_clean.iloc[:,-1]

"""## Menyeimbangkan Data"""

# pengecekan persebaran jumlah target

df_clean['target'].value_counts().plot(kind='bar',
                                       figsize=(10,6),
                                       color=['red', 'green', 'blue', 'yellow', 'purple'])
plt.title("Count of the target")
plt.xticks(rotation=0);

"""Dapat dilihat bahwa pada grafik diatas menunjukkan persebaran jumlah pada target tidak seimbang, sehingga perlu dilakukan penyeimbangan data dengan melakukan oversampling. Oversampling dilakukan karena jumlah dataset pada proyek ini sedikit. Metode yang digunakan yaitu SMOTE (Synthetic Minority Oversampling Technique)."""

# mengimpor kelas SMOTE yang ada dalam pustaka imblearn yaitu pustaka Python yang menawarkan teknik untuk mengatasi masalah ketidakseimbangan kelas dalam dataset

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
x_smote_resampled, y_smote_resampled = smote.fit_resample(x, y)

# membandingkan target yang telah dilakukan oversampling dan target yang belum dilakukan oversampling

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
new_df1 = pd.DataFrame(data=y)
new_df1.value_counts().plot(kind='bar',figsize=(10,6),color=['red','green','blue','yellow', 'purple'])
plt.title("Target Before Oversampling with SMOTE")
plt.xticks(rotation=0);

plt.subplot(1, 2, 2)
new_df2 = pd.DataFrame(data=y_smote_resampled)
new_df2.value_counts().plot(kind='bar',figsize=(10,6),color=['red','green','blue','yellow', 'purple'])
plt.title("Target After Oversampling with SMOTE")
plt.xticks(rotation=0);

plt.tight_layout()
plt.show()

"""Dari pembandingan grafik yang ditampilkan, terlihat bahwa penerapan teknik Oversampling SMOTE telah berhasil menyeimbangkan distribusi data secara efektif.

## Normalisasi Data
"""

df_clean.describe()

"""Dari analisis deskriptif dataset, terlihat bahwa terdapat variasi yang signifikan dalam standar deviasi, atau simpangan baku, di antara fitur-fitur dalam dataset. Oleh karena itu, diperlukan proses normalisasi untuk mengurangi perbedaan rentang nilai standar deviasi antar fitur tersebut dengan menggunakan MinMaxScaler. MinMaxScaler digunakan untuk mengubah fitur dengan menskalakan setiap fitur ke rentang antara 0 dan 1 atau -1 dan 1."""

# mengimpor MinMaxScaler dari modul preprocessing yang ada dalam pustaka Scikit-learn

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x_smote_resampled_normal = scaler.fit_transform(x_smote_resampled)

len(x_smote_resampled_normal)

df_check1 = pd.DataFrame(x_smote_resampled_normal)
df_check1.describe()

"""Setelah dilakukan normalisasi pada dataset dapat terlihat bahwa rentang pada standar deviasi setiap fitur dataset menjadi lebih sama dan tidak terlalu jauh.

## Membagi Data
"""

from sklearn.model_selection import train_test_split

# membagi fitur dan target menjadi data train dan test untuk dataset yang sudah di oversampling

x_train, x_test, y_train, y_test = train_test_split(x_smote_resampled, y_smote_resampled, test_size=0.2, random_state=42,stratify=y_smote_resampled)

# membagi fitur dan target menjadi data train dan test untuk dataset yang sudah dilakukan oversampling dan normalisasi

x_train_normal, x_test_normal, y_train_normal, y_test_normal = train_test_split(x_smote_resampled_normal, y_smote_resampled, test_size=0.2, random_state=42,stratify = y_smote_resampled)

"""#7) Membangun Model

Tahapan berikutnya yaitu membangun model dengan memilih beberapa algorima untuk prediksi penyakit yaitu:
1. KNN (K-Nearest Neighbors)
2. Random Forest
3. XGBoost
"""

from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,roc_auc_score,confusion_matrix,precision_score

"""Membuat fungsi evaluasi untuk perhitungan hasil akurasi dan rata-rata dari recall, f1, dan precision score dari setiap model."""

def evaluation(Y_test,Y_pred):
    acc = accuracy_score(Y_test,Y_pred)
    rcl = recall_score(Y_test,Y_pred,average = 'weighted')
    f1 = f1_score(Y_test,Y_pred,average = 'weighted')
    ps = precision_score(Y_test,Y_pred,average = 'weighted')

    metric_dict={'accuracy': round(acc,3),
               'recall': round(rcl,3),
               'F1 score': round(f1,3),
               'Precision score': round(ps,3)
              }

    return print(metric_dict)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

"""## Data Oversampling

### KNN

Membangun model menggunakan algoritma KNN dengan nilai neighbors 3, alasan pengggunaan nilai neighbors 3 digunakan untuk menghindari kebuntuan, maka nilai ganjil lebih baik digunakan.
"""

knn_model = KNeighborsClassifier(n_neighbors = 3)
knn_model.fit(x_train, y_train)

y_pred_knn = knn_model.predict(x_test)

# Evaluate the KNN model
print("K-Nearest Neighbors (KNN) Model:")
accuracy_knn_smote = round(accuracy_score(y_test,y_pred_knn),3)
print("Accuracy:", accuracy_knn_smote)
print("Classification Report:")
print(classification_report(y_test, y_pred_knn))

evaluation(y_test,y_pred_knn)

"""Menampilkan confusion matrix untuk membandingkan nilai prediksi model dengan nilai yang sesungguhnya."""

cm = confusion_matrix(y_test, y_pred_knn)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""### Random Forest

Membangun model menggunakan algoritma Random Forest dengan jumlah n_estimators sebesar 100. Di mana n_estimators berguna untuk mengatur jumlah pohon keputusan yang dibangun.
"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(x_train, y_train)

y_pred_rf = rf_model.predict(x_test)

# Evaluate the Random Forest model
print("\nRandom Forest Model:")
accuracy_rf_smote = round(accuracy_score(y_test, y_pred_rf),3)
print("Accuracy:",accuracy_rf_smote)
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))

evaluation(y_test,y_pred_rf)

cm = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""### XGBoost

Membangun model menggunakan XGBoost dengan learning rate sebesar 0,1. Di mana learning rate digunakan untuk mengontrol seberapa besar dalam menyesuaikan bobot model.
"""

xgb_model = XGBClassifier(learning_rate=0.1, n_estimators=100, random_state=42)
xgb_model.fit(x_train, y_train)

y_pred_xgb = xgb_model.predict(x_test)

# Evaluate the XGBoost model
print("\nXGBoost Model:")
accuracy_xgb_smote = round(accuracy_score(y_test, y_pred_xgb),3)
print("Accuracy:",accuracy_xgb_smote)
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb))

evaluation(y_test,y_pred_xgb)

cm = confusion_matrix(y_test, y_pred_xgb)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""## Data Oversampling + Normalisasi

### KNN
"""

knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(x_train_normal, y_train_normal)

y_pred_knn = knn_model.predict(x_test_normal)

# Evaluate the KNN model
print("K-Nearest Neighbors (KNN) Model:")
accuracy_knn_smote_normal = round(accuracy_score(y_test_normal,y_pred_knn),3)
print("Accuracy:", accuracy_knn_smote_normal)
print("Classification Report:")
print(classification_report(y_test_normal, y_pred_knn))

evaluation(y_test_normal,y_pred_knn)

cm = confusion_matrix(y_test_normal, y_pred_knn)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""### Random Forest"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(x_train_normal, y_train_normal)

y_pred_rf = rf_model.predict(x_test_normal)

# Evaluate the Random Forest model
print("\nRandom Forest Model:")
accuracy_rf_smote_normal = round(accuracy_score(y_test_normal, y_pred_rf),3)
print("Accuracy:",accuracy_rf_smote_normal )
print("Classification Report:")
print(classification_report(y_test_normal, y_pred_rf))

evaluation(y_test_normal,y_pred_rf)

cm = confusion_matrix(y_test_normal, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""### XGBoost"""

xgb_model = XGBClassifier(learning_rate=0.1, n_estimators=100, random_state=42)
xgb_model.fit(x_train_normal, y_train_normal)

y_pred_xgb = xgb_model.predict(x_test_normal)

# Evaluate the XGBoost model
print("\nXGBoost Model:")
accuracy_xgb_smote_normal = round(accuracy_score(y_test_normal, y_pred_xgb),3)
print("Accuracy:",accuracy_xgb_smote_normal)
print("Classification Report:")
print(classification_report(y_test_normal, y_pred_xgb))

evaluation(y_test_normal,y_pred_xgb)

cm = confusion_matrix(y_test_normal, y_pred_xgb)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""## Data Tunning + Oversampling + Normalisasi

Tunning Data merupakan proses penyesuaian dan optimasi untuk meningkatkan kinerja dan efektivitas model. Salah satunya dengan memastikan data yang digunakan untuk training dan testing model adalah representasi terbaik dari masalah yang akan dipecahkan dan diharapkan dengan Tunning Data dapat mengembangkan model prediksi dengan lebih akurat. Pada proyek ini Tunning Data menggunakan RandomizedSearchCV dari pustaka scikit-learn.
"""

# tunning data

from sklearn.model_selection import RandomizedSearchCV

"""### KNN"""

knn_model = KNeighborsClassifier()

param_grid = {
    "n_neighbors": range(3, 21),
    "metric": ["euclidean", "manhattan", "chebyshev"],
    "weights": ["uniform", "distance"],
    "algorithm": ["auto", "ball_tree", "kd_tree"],
    "leaf_size": range(10, 61),
}

knn_model = RandomizedSearchCV(estimator=knn_model, param_distributions=param_grid, n_iter=100, scoring="accuracy", cv=5)


knn_model.fit(x_train_normal, y_train_normal)

best_params = knn_model.best_params_
print(f"Best parameters: {best_params}")

y_pred_knn = knn_model.predict(x_test_normal)

# Evaluate the KNN model
print("K-Nearest Neighbors (KNN) Model:")
accuracy_knn_smote_normal_Tun = round(accuracy_score(y_test_normal,y_pred_knn),3)
print("Accuracy:", accuracy_knn_smote_normal_Tun)
print("Classification Report:")
print(classification_report(y_test_normal, y_pred_knn))

evaluation(y_test_normal,y_pred_knn)

cm = confusion_matrix(y_test_normal, y_pred_knn)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""### Random Forest"""

rf_model = RandomForestClassifier()

param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [ 10, 15],
    "min_samples_leaf": [1, 2],
    "min_samples_split": [2, 5],
    "max_features": ["sqrt", "log2"],
    # "random_state": [42, 100, 200]
}

rf_model = RandomizedSearchCV(rf_model, param_grid, n_iter=100, cv=5, n_jobs=-1)

rf_model.fit(x_train_normal, y_train_normal)

best_params = rf_model.best_params_
print(f"Best parameters: {best_params}")

y_pred_rf = rf_model.predict(x_test_normal)

# Evaluate the Random Forest model
print("\nRandom Forest Model:")
accuracy_rf_smote_normal_Tun = round(accuracy_score(y_test_normal, y_pred_rf),3)
print("Accuracy:",accuracy_rf_smote_normal_Tun)
print("Classification Report:")
print(classification_report(y_test_normal, y_pred_rf))

evaluation(y_test_normal,y_pred_rf)

cm = confusion_matrix(y_test_normal, y_pred_knn)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""### XGBoost"""

xgb_model = XGBClassifier()

param_grid = {
    "max_depth": [3, 5, 7],
    "learning_rate": [0.01, 0.1],
    "n_estimators": [100, 200],
    "gamma": [0, 0.1],
    "colsample_bytree": [0.7, 0.8],
}

xgb_model = RandomizedSearchCV(xgb_model, param_grid, n_iter=10, cv=5, n_jobs=-1)


xgb_model.fit(x_train_normal, y_train_normal)

best_params = xgb_model.best_params_
print(f"Best parameters: {best_params}")

y_pred_xgb = xgb_model.predict(x_test_normal)

# Evaluate the XGBoost model
print("\nXGBoost Model:")
accuracy_xgb_smote_normal_Tun = round(accuracy_score(y_test_normal, y_pred_xgb),3)
print("Accuracy:",accuracy_xgb_smote_normal_Tun)
print("Classification Report:")
print(classification_report(y_test_normal, y_pred_xgb))

evaluation(y_test_normal,y_pred_xgb)

cm = confusion_matrix(y_test_normal, y_pred_xgb)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('True')
plt.ylabel('Predict')
plt.show()

"""#8) Evaluasi Model

Tahapan selanjutnya evaluasi model dengan membandingkan beberapa algoritma yang telah diuji coba dengan tujuan untuk mengetahui algoritma yang menghasilkan prediksi yang paling akurat.
"""

model_comp1 = pd.DataFrame({'Model': ['K-Nearest Neighbour','Random Forest','XGBoost'],
                            'Accuracy': [accuracy_knn_smote*100, accuracy_rf_smote*100, accuracy_xgb_smote*100]})
model_comp1.head()

# Membuat bar plot dengan keterangan jumlah
fig, ax = plt.subplots()
bars = plt.bar(model_comp1['Model'], model_comp1['Accuracy'], color=['red', 'yellow', 'green'])
plt.xlabel('Model')
plt.ylabel('Accuracy (%)')
plt.title('Oversampling')
plt.xticks(rotation=45, ha='right')  # Untuk memutar label sumbu x agar lebih mudah dibaca

# Menambahkan keterangan jumlah di atas setiap bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.show()

model_comp2 = pd.DataFrame({'Model': ['K-Nearest Neighbour','Random Forest','XGBoost'],
                            'Accuracy': [accuracy_knn_smote_normal*100,accuracy_rf_smote_normal*100,accuracy_xgb_smote_normal*100]})
model_comp2.head()

# Membuat bar plot dengan keterangan jumlah
fig, ax = plt.subplots()
bars = plt.bar(model_comp2['Model'], model_comp2['Accuracy'], color=['red', 'yellow', 'green'])
plt.xlabel('Model')
plt.ylabel('Accuracy (%)')
plt.title('Oversampling + Normalisasi')
plt.xticks(rotation=45, ha='right')  # Untuk memutar label sumbu x agar lebih mudah dibaca

# Menambahkan keterangan jumlah di atas setiap bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.show()

model_comp3 = pd.DataFrame({'Model': ['K-Nearest Neighbour','Random Forest','XGBoost'],
                            'Accuracy': [accuracy_knn_smote_normal_Tun*100,accuracy_rf_smote_normal_Tun*100,accuracy_xgb_smote_normal_Tun*100]})
model_comp3.head()

# Membuat bar plot dengan keterangan jumlah
fig, ax = plt.subplots()
bars = plt.bar(model_comp3['Model'], model_comp3['Accuracy'], color=['red', 'yellow', 'green'])
plt.xlabel('Model')
plt.ylabel('Accuracy (%)')
plt.title('Normalization + Oversampling + Tunning')
plt.xticks(rotation=45, ha='right')  # Untuk memutar label sumbu x agar lebih mudah dibaca

# Menambahkan keterangan jumlah di atas setiap bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.show()

model_compBest = pd.DataFrame({'Model': ['K-Nearest Neighbour OverSample Tunning', 'Random Forest OverSample','XGB OverSample Standarization Tunning'],
                               'Accuracy': [accuracy_knn_smote_normal_Tun*100, accuracy_rf_smote_normal*100,accuracy_xgb_smote_normal_Tun*100]
})

# Membuat bar plot dengan keterangan jumlah
fig, ax = plt.subplots()
bars = plt.bar(model_compBest['Model'], model_compBest['Accuracy'], color=['red', 'yellow', 'green'])
plt.xlabel('Model')
plt.ylabel('Accuracy (%)')
plt.title('Best Model Comparison')
plt.xticks(rotation=45, ha='right')  # Untuk memutar label sumbu x agar lebih mudah dibaca

# Menambahkan keterangan jumlah di atas setiap bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.show()

"""#9) Kesimpulan

Berdasarkan hasil uji coba pada dataset yang hanya dilakukan oversampling dapat dilihat bahwa model dengan menggunakan algoritma Random Forest dan XGBoost mempertahankan performa yang hampir sama dengan akurasi masing-masing 92% dan 90.4%, tetapi KNN menunjukkan penurunan yang signifikan dalam akurasi sebesar 75.4%. Selanjutnya pada dataset yang dilakukan oversampling+normalisasi dapat dilihat bahwa model dengan algoritma Random Forest dapat memberikan performa terbaik dengan akurasi sebesar 92%, diikuti dengan algoritma XGBoost sebesar 90.4% dan KNN sebesar 86.1%. Terakhir, uji coba pada dataset yang dilakukan oversampling+normalisasi+tuning dapat dilihat bahwa ternyata model dengan algoritma KNN memiliki akurasi tertinggi sebesar 92%, yang kemudian diikuti oleh algoritma XGBoost dan Random Forest dengan akurasi masing-masing sebesar 89.8% dan 90.4%.

Dari hasil uji coba yang telah dilakukan dapat disimpulkan bahwa:
1. Penambahan proses tuning setelah dilakukan normalisasi dan oversampling dapat memberikan peningkatan performa untuk model yang menggunakan algoritma KNN dan XGBoost
2. Model yang menggunakan algoritma Random Forest dan XGBoost menunjukkan kestabilan dalam performa tanpa perlu dilakukan proses tuning, terutama pada saat dataset telah dilakukan normalisasi+oversampling
"""